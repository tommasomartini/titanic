{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees\n",
    "Use decision trees to solve the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset as ds\n",
    "\n",
    "train_df, eval_df = ds.get_formatted_splits()\n",
    "train_df.dtypes\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: likelihood on gender\n",
    "Let's try something naive: predict the probability of surviving given a binary variable like gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_of_surviving_based_on_gender(df):\n",
    "    prob_survival_on_gender = df.dropna(subset=['Sex'], inplace=False).groupby('Sex').agg({'Survived': ['mean', 'count']}).rename(columns={'mean': 'Prob_surviving', 'count': 'Total'})\n",
    "    prob_survival_on_gender.columns = prob_survival_on_gender.columns.droplevel(0)\n",
    "    prob_survival_on_gender = prob_survival_on_gender.assign(Fraction=prob_survival_on_gender['Total'] / prob_survival_on_gender['Total'].sum())\n",
    "    return prob_survival_on_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_survival_on_gender_train = get_prob_of_surviving_based_on_gender(train_df)\n",
    "prob_survival_on_gender_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to apply these probabilities to the evaluation data: if the probability t survive is higher than 0.5, then we classify that passenger as \"survived\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_survival_on_gender_train['Prediction'] = prob_survival_on_gender_train['Prob_surviving'] > 0.5\n",
    "prob_survival_on_gender_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gender_likelihood_on_set(df):\n",
    "    predictions = df['Sex'].apply(lambda gender: prob_survival_on_gender_train.loc[gender]['Prediction'])\n",
    "    accuracy = (predictions == df['Survived']).mean()\n",
    "    return accuracy\n",
    "\n",
    "train_accuracy = evaluate_gender_likelihood_on_set(train_df)\n",
    "print('Accuracy on training set: {}'.format(train_accuracy))\n",
    "\n",
    "eval_accuracy = evaluate_gender_likelihood_on_set(eval_df)\n",
    "print('Accuracy on evaluation set: {}'.format(eval_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: decision tree on gender\n",
    "Just to double check: we should get the same results as approach 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "gender_encoder = LabelEncoder().fit(train_df['Sex'])\n",
    "tree_classifier = DecisionTreeClassifier().fit(gender_encoder.transform(train_df['Sex']).reshape(-1, 1), train_df['Survived'])\n",
    "\n",
    "accuracy = tree_classifier.score(gender_encoder.transform(eval_df['Sex']).reshape(-1, 1), eval_df['Survived'])\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: decision tree on single features\n",
    "This cannot possibly be better than combining all the attributes, but I am curious to see which one is more significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_classification_on_attribute(attribute_name):\n",
    "    label_encoder = LabelEncoder().fit(train_df[attribute_name].dtype.categories)\n",
    "    encoded_attributes = label_encoder.transform(train_df[attribute_name])\n",
    "    dtc = DecisionTreeClassifier().fit(encoded_attributes.reshape(-1, 1), train_df['Survived'])\n",
    "    acc = dtc.score(label_encoder.transform(eval_df[attribute_name]).reshape(-1, 1), eval_df['Survived'])\n",
    "    return acc\n",
    "\n",
    "# Try on categorical attributes first.\n",
    "categorical_columns = ['Sex', 'Pclass', 'Embarked', 'Title', 'Floor']\n",
    "for attribute_name in categorical_columns:\n",
    "    print('Accuracy on {}: {}'.format(attribute_name, tree_classification_on_attribute(attribute_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 4: tree combining the categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier cannot handle categorical data, so we need to turn all the categorical columns to numerical.\n",
    "dtc = DecisionTreeClassifier().fit(train_df[categorical_columns].apply(lambda x: x.cat.codes), train_df['Survived'])\n",
    "acc = dtc.score(eval_df[categorical_columns].apply(lambda x: x.cat.codes), eval_df['Survived'])\n",
    "print('Accuracy combining categorical attributes: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Quick experiment on tree parameters.\n",
    "def _compute_dtc_score(criterion, max_depth, class_weight):\n",
    "    return DecisionTreeClassifier(class_weight=class_weight,\n",
    "                                  criterion=criterion,\n",
    "                                  max_depth=max_depth)\\\n",
    "            .fit(train_df[categorical_columns].apply(lambda x: x.cat.codes),\n",
    "                train_df['Survived'])\\\n",
    "            .score(eval_df[categorical_columns].apply(lambda x: x.cat.codes),\n",
    "                  eval_df['Survived'])\n",
    "\n",
    "criteria = ['entropy', 'gini']\n",
    "max_allowed_depth = 20\n",
    "accuracies = np.zeros((2, len(criteria), max_allowed_depth))\n",
    "for idx0, class_weight in enumerate([None, 'balanced']):\n",
    "    for idx1, criterion in enumerate(criteria):\n",
    "        for idx2, max_depth in enumerate(range(1, max_allowed_depth + 1)):\n",
    "            accuracies[idx0, idx1, idx2] = _compute_dtc_score(criterion, max_depth, class_weight)\n",
    "\n",
    "dt_plot = figure()\n",
    "dt_plot.line(range(1, max_allowed_depth + 1), accuracies[0, 0, :], color='blue', legend_label='Unbalanced {}'.format(criteria[0]))\n",
    "dt_plot.line(range(1, max_allowed_depth + 1), accuracies[0, 1, :], color='orange', legend_label='Unbalanced {}'.format(criteria[1]))\n",
    "dt_plot.line(range(1, max_allowed_depth + 1), accuracies[1, 0, :], color='red', legend_label='Balanced {}'.format(criteria[0]))\n",
    "dt_plot.line(range(1, max_allowed_depth + 1), accuracies[1, 1, :], color='green', legend_label='Balanced {}'.format(criteria[1]))\n",
    "dt_plot.xaxis.axis_label = 'Max depth'\n",
    "dt_plot.yaxis.axis_label = 'DTC accuracy'\n",
    "show(dt_plot)\n",
    "\n",
    "print('Max accuracy: {}'.format(np.max(accuracies)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
